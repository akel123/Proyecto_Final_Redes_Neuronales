\section{Introducción}

Las redes neuronales profundas (DNNs) han ganado una alta reputación con respecto a sus capacidades de clasificar imagenes igual (o hasta superior) que los humanos. Pero en el pasado reciente, ha quedado cada vez más claro que las redes aprenden clasificar de manera muy distinta que los humanos. Una de las propiedades que realmente demostró eso fue el descrubrimiento de su suceptibilidad a los ataques adversarias \cite{szegedy2014intriguing}. Esos ataques se construyen agregandoles a las imagenes una pequeña perturbación imperceptibles para los humanos que engañan a la red. Es decir, aunque una imagen adversaria se parezca igual a la original, la red se equivoca con la clasificación de la adversaria con alta probabilidad. Como los ataques pueden no ser detectables por los humans, se plantea la preocupación que puedan ser usados maliciosamente; por ejemplo, en la tecnología de reconocimiento de imágenes que se utiliza en los automóviles autónomos. Por eso se requiere mas profundizamiento del conocimiento asociado. 

A grandes rasgos, los ataques pueden dividirse entre dos categorías: dirigidos y no-dirigidos. Primero hablemos de los ataques dirigidos; muchos usan el gradiente de manera directa. La idea es que se toma el gradiente de la función de pérdida con respecto a la imagen, y eso se usa para diseñar una perturbación que maximiza el error de la clasificación cuando se le agrega a la imagen. Dos ejemplos de ataques no-dirigidos que utilizan el gradiente directamente son el projected gradient decent (PGD) \cite{madry2019deep} y el fast gradient sign method (FGSM) \cite{goodfellow2015explaining}. Este último saca el signo de cada elemento del gradiente en lugar de usar el gradiente verdader; eso hace que sea más rapido con grandes cantidades de datos. En este artículo se usan los dos de PDG y FGSM para explorar los ataques no-dirigidos. El objectivo de los ataques no dirigidos es que cambien la clasificación correcta a cualquier otra ataque, mientras que los ataques dirigidos tienen una deseada a la que cambian la clasificación correcta. Se examina el ataque Carlini \& Wager (CW), el cual puede actuar como dirigio o no-dirigido. 

Cabe mencionar la diferencia entre los ataques caja blanca y caja negra. En el primero todos los detalles de la red (pesos, arquitectura, etc) son conocidos por el atacante. Al contrario, en los caja blanca los detalles son escondidos, y solo pueden saberse las entradas y salidas. Aunque parezca muy difícil, los ataques caja negra son bastante existosos por la facilidad de aproximar el gradiente solo por las entradas y las salidas. Los ataques que se mencionan en este artículo son de los caja blanca.

Desde el descrubrimiento de ese vulnerabilidad que tienen las DNN, se han hecho defensas para tratar de combatar los ataques. Aunque ninguna defensa funciona para completamtente resistir los ataques, muchas sí tienen effecto, y vale la pena explorar cuales propiedades contribuyen a su exito. Igual que los ataques, las defensas también se pueden categorizar en dos modalidades. Las del primer tipo modifican el entrenamiento de la red para que la función de pérdida se vuelva más suave. Entre más suave esa función, más difícil será para que las perturbaciones pequeñas hagan cambios grandes. Un ejemplo conocido de este tipo es el entrenamiento adversario \cite{goodfellow2015explaining, Shaham_2018, szegedy2014intriguing}. Las del otro tipo de defensa, y las que vamos a estudiar en este paper, no modifican el entrenamiento ni la arquitectura, sino le ponen un preprocesamiento a las imagenes de entrada. La defensa que se va a utilizar en ese reporte es compresión JPEG \cite{das2017keeping}.




