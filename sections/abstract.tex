\section*{Abstract}

\selectlanguage{english}

The increasing use of artificial neural networks (ANNs) in security areas makes it imperative to understand the way in which these attacks act and why they are so potent. Thus, in this work we attempt to shed some light on this by experimenting and analyzing the effects that attacks, defenses, and inherent properties of the networks (such as overfitting, overparameterization, and transfer function) may have on the ability of the network to resist attacks. In addition to studying the attacks via frequency analysis, we also consider them in the context of saliency (changes to which pixels have the most effect in the classification?). Our finding suggests that as image complexity increases (MNIST vs CIFAR-10), the attacks become less noticeable and difficult to interpret.  We also found some supporting evidence for the hypothesis that networks that show more resistance to adversarial attacks tend to have a transfer function more similar to that of humans (band pass). However, there is still a lot to understand and develop in order to defend ANNs adequately. \\[1cm]

\noindent\textbf{Keywords}: neural networks, adversarial attacks, JPEG defense, saliency, overfitting, overparameterization, transfer function, 2D FFT