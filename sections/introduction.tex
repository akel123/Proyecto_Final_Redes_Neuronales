\section{Introducción}

Las redes neuronales profundas (DNNs) han ganado una alta reputación con respecto a sus capacidades de clasificar imágenes con tanta precisión (y en algunos casos con una precisión incluso superior) como la de los humanos. Pero es cada vez más claro que las redes aprenden a clasificar de una manera muy distinta a la de los humanos. Una de las propiedades que demostró eso fue el descubrimiento de su susceptibilidad a los ataques adversarios \cite{szegedy2014intriguing}. Estos ataques se construyen agregándoles a las imágenes una pequeña perturbación, imperceptible para los humanos, pero que engaña a la red, es decir, aunque una imagen adversaria para nosotros parezca igual a la original, la red tiene una alta probabilidad de clasificar mal a la imagen adversaria. Dado que los ataques, en general, no son detectables por los humanos, se plantea la preocupación de que puedan ser usados maliciosamente; por ejemplo, en la tecnología de reconocimiento de imágenes que se utiliza en los automóviles autónomos, por eso se requiere más profundizar en el conocimiento asociado a estos ataques adversarios.

A grandes rasgos, los ataques pueden dividirse en dos categorías: dirigidos y no dirigidos. El objetivo de los ataques no dirigidos es que cambien la clasificación correcta por cualquier otra, mientras que los ataques dirigidos buscan cambiar la clasificación correcta por una clasificación específica. En el caso de los ataques no dirigidos, generalmente se usa el gradiente de manera directa: se toma el gradiente de la función de pérdida con respecto a la imagen y eso se usa para diseñar una perturbación que maximice el error de la clasificación cuando se le agrega a la imagen. Dos ejemplos de ataques no dirigidos que utilizan el gradiente directamente son el \textbf{projected gradient descent} (PGD) \cite{madry2019deep} y el \textbf{fast gradient sign method} (FGSM) \cite{goodfellow2015explaining}. Este último toma el signo de cada elemento del gradiente en lugar de usar el valor del gradiente, esto hace que sea más rápido con grandes cantidades de datos. En este trabajo utilizamos el \textbf{fast gradient method} FGM (con norma $L_2$), el FGSM (FGM con norma $L_\infty$) y el ataque con norma $L_2$ de Carlini y Wagner (CW) para explorar los ataques no-dirigidos. El ataque de Carlini y Wager puede ser dirigido o no-dirigido. 

Cabe mencionar la diferencia entre los ataques de caja blanca y caja negra. En el primero, todos los detalles de la red (pesos, arquitectura, etc.) son conocidos por el atacante. Por el contrario, en los ataques de caja blanca, los detalles están escondidos, y solo se conocen las entradas (inputs) y las salidas (outputs). Aunque parezca muy difícil, los ataques de caja negra son bastante exitosos por la facilidad de aproximar el gradiente solo por las entradas y las salidas. Los ataques que se emplean en este artículo son los de caja blanca.

Desde el descubrimiento de la vulnerabilidad que tienen las DNNs ante los ataques adversarios, se han diseñado defensas para tratar de combatir los ataques. Aunque ninguna defensa funciona para resistir completamente los ataques, muchas sí tienen efecto, y vale la pena explorar qué propiedades contribuyen a su éxito. Al igual que los ataques, las defensas también se pueden categorizar en dos modalidades. Las del primer tipo modifican el entrenamiento de la red para que la función de pérdida se vuelva más suave. Entre más suave sea esa función, más difícil será que las perturbaciones pequeñas hagan grandes cambios. Un ejemplo conocido de este tipo es el entrenamiento adversario \cite{goodfellow2015explaining, Shaham_2018, szegedy2014intriguing}. Las defensas del segundo tipo no modifican el entrenamiento ni la arquitectura, sino que utilizan un preprocesamiento en las imágenes de entrada. La defensa que se utiliza en este artículo corresponde al segundo tipo y es la compresión JPEG \cite{das2017keeping}.

En este trabajo estudiamos la eficacia de la compresión JPEG en contra de los ataques y estudiamos el efecto que tienen el overfitting y la overparameterization en las redes y, dado que nuestro objetivo fue encontrar una medida de interpretabilidad de los ataques adversarios, empleamos análisis de frecuencias para observar si las redes funcionan como un filtro en el sentido de teoría de control, específicamente esperando encontrar un comportamiento del tipo pasabandas, ya que esa es más o menos la manera en que nuestra mente clasifica imágenes, y analizamos la prominencia (saliency) \cite{simonyan2014deep} de las imágenes para saber cuáles son las regiones en las que las redes se enfocan cuando analizan las imágenes.